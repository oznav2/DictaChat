services:
  redis:
    image: redis:7.2-alpine
    container_name: bricksllm-redis
    restart: unless-stopped
    ports:
      - '${REDIS_HOST_PORT:-6380}:6379'
    command: redis-server --save 20 1 --loglevel warning --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - bricksllm-network
    healthcheck:
      test: ['CMD', 'redis-cli', '--raw', 'incr', 'ping']
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  mongo:
    image: mongo:7
    container_name: bricksllm-mongo
    restart: unless-stopped
    ports:
      - '27017:27017'
    volumes:
      - mongo_data:/data/db
    networks:
      - bricksllm-network
    healthcheck:
      test: ['CMD', 'mongosh', '--eval', 'db.adminCommand("ping")']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  postgresql:
    image: postgres:16.1-alpine
    container_name: bricksllm-postgresql
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRESQL_USERNAME:-postgres}
      POSTGRES_PASSWORD: ${POSTGRESQL_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRESQL_DB:-bricksllm}
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - '${POSTGRESQL_HOST_PORT:-5433}:5432'
    volumes:
      - postgresql_data:/var/lib/postgresql/data
    networks:
      - bricksllm-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRESQL_USERNAME:-postgres} -d ${POSTGRESQL_DB:-bricksllm}"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  llama-server:
    image: ${LLAMA_IMAGE}
    container_name: bricksllm-llama
    restart: unless-stopped
    runtime: nvidia
    environment:
      CUDA_VISIBLE_DEVICES: ${GPU_DEVICE_IDS:-0}
      SYSTEM_PROMPT: ${SYSTEM_PROMPT}
    ports:
      - '${LLAMA_HOST_PORT:-5002}:5002'
    volumes:
      - ${LOCAL_MODEL_PATH:-./models}:/models
      - ./llama_entrypoint.sh:/app/entrypoint.sh
      - ./chat_template.jinja2.template:/app/chat_template.jinja2.template
    networks:
      - bricksllm-network
    entrypoint: ["/app/entrypoint.sh", "/app/llama-server"]
    command:
      - -m
      - /models/${HF_FILE}
      - -c
      - '${CONTEXT_SIZE:-8192}'
      - --host
      - '0.0.0.0'
      - --port
      - '5002'
      - --n-gpu-layers
      - '${N_GPU_LAYERS:-99}'
      - --temp
      - '${TEMPERATURE:-0.7}'
      - --top-p
      - '${TOP_P:-0.9}'
      - --repeat-penalty
      - '${REPEAT_PENALTY:-1.1}'
      - -n
      - '${NUM_PREDICT:-2048}'
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:5002/health']
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]


  bricksllm:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: bricksllm-gateway
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
      postgresql:
        condition: service_healthy
      # llama-server:
      #   condition: service_healthy
    environment:
      POSTGRESQL_HOSTS: postgresql
      POSTGRESQL_PORT: 5432
      POSTGRESQL_DB_NAME: ${POSTGRESQL_DB:-bricksllm}
      POSTGRESQL_USERNAME: ${POSTGRESQL_USERNAME:-postgres}
      POSTGRESQL_PASSWORD: ${POSTGRESQL_PASSWORD:-postgres}
      POSTGRESQL_READ_TIME_OUT: ${POSTGRESQL_READ_TIME_OUT:-2s}
      POSTGRESQL_WRITE_TIME_OUT: ${POSTGRESQL_WRITE_TIME_OUT:-1s}
      REDIS_HOSTS: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      REDIS_READ_TIME_OUT: ${REDIS_READ_TIME_OUT:-1s}
      REDIS_WRITE_TIME_OUT: ${REDIS_WRITE_TIME_OUT:-1s}
      IN_MEMORY_DB_UPDATE_INTERVAL: ${IN_MEMORY_DB_UPDATE_INTERVAL:-5s}
      STATS_PROVIDER: ${STATS_PROVIDER:-}
      AMAZON_SECRET_ARN: ${AMAZON_SECRET_ARN:-}
      AMAZON_REGION: ${AMAZON_REGION:-}
      PROXY_TIMEOUT: ${PROXY_TIMEOUT:-600s}
      NUMBER_OF_EVENT_MESSAGE_CONSUMERS: ${NUMBER_OF_EVENT_MESSAGE_CONSUMERS:-3}
    ports:
      - '${BRICKSLLM_ADMIN_PORT:-8001}:8001'
      - '${BRICKSLLM_PROXY_PORT:-8002}:8002'
    volumes:
      - bricksllm_logs:/var/log/bricksllm
    networks:
      - bricksllm-network
    healthcheck:
      test: ['CMD', 'wget', '--no-verbose', '--tries=1', '--spider', 'http://localhost:8001/api/health']
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    command: ['-m', '${BRICKSLLM_MODE:-production}']

  swagger-ui:
    image: swaggerapi/swagger-ui
    container_name: bricksllm-swagger
    restart: unless-stopped
    environment:
      SWAGGER_JSON: /docs/admin.yaml
    volumes:
      - ./docs/admin.yaml:/docs/admin.yaml
    ports:
      - '${SWAGGER_HOST_PORT:-8082}:8080'
    networks:
      - bricksllm-network

  frontend-ui:
    build:
      context: ./frontend-huggingface
      dockerfile: Dockerfile
    container_name: frontend-UI
    restart: unless-stopped
    environment:
      - PUBLIC_APP_NAME=DictaLM Chat
      - PUBLIC_APP_ASSETS=chat-ui
      - PUBLIC_APP_COLOR=blue
      - PUBLIC_APP_DESCRIPTION=A chat interface for DictaLM
      - PUBLIC_APP_DATA_SHARING=1
      - PUBLIC_APP_DISCLAIMER=0
      - MONGODB_URL=mongodb://mongo:27017/chat-ui
      - HF_TOKEN=${HF_TOKEN:-}
      - DOCKER_ENV=true
      - OPENAI_BASE_URL=http://bricksllm:8002/api/custom/providers/llama-cpp-root
      - OPENAI_API_KEY=${BRICKSLLM_API_KEY:-sk-bricksllm-frontend-llama-key-explicit}
      - ENABLE_CONFIG_MANAGER=false
      - ORIGIN=http://localhost:8004
      - LLM_ROUTER_ENABLE_TOOLS=true
      - LLM_ROUTER_TOOLS_MODEL=dictalm-3.0-24b-thinking-fp8-q4_k_m.gguf
      - 'MODELS=[{"id":"dictalm-3.0-24b-thinking-fp8-q4_k_m.gguf","name":"DictaLM 3.0 24B","endpoints":[{"type":"openai","baseURL":"http://bricksllm:8002/api/custom/providers/llama-cpp-root"}],"supportsTools":true}]'
      - 'MCP_SERVERS=[{"name":"Everything","url":"http://mcp-sse-proxy:3100/everything/sse"},{"name":"Context7","url":"https://mcp.context7.com/mcp"},{"name":"Docker","url":"http://mcp-sse-proxy:3100/docker/sse"},{"name":"Sequential Thinking","url":"http://mcp-sse-proxy:3100/sequential-thinking/sse"},{"name":"Git","url":"http://mcp-sse-proxy:3100/git/sse"},{"name":"Fetch","url":"http://mcp-sse-proxy:3100/fetch/sse"},{"name":"Time","url":"http://mcp-sse-proxy:3100/time/sse"},{"name":"Memory","url":"http://mcp-sse-proxy:3100/memory/sse"},{"name":"Filesystem","url":"http://mcp-sse-proxy:3100/filesystem/sse"},{"name":"Perplexity","url":"http://mcp-sse-proxy:3100/perplexity/sse"},{"name":"Tavily Search","url":"http://mcp-sse-proxy:3100/Tavily/sse"},{"name":"YouTube Summarizer","url":"http://mcp-sse-proxy:3100/youtube-video-summarizer/sse"}]'
    ports:
      - "8004:3000"
    volumes:
      - ./frontend-huggingface:/app
      - /app/node_modules
    depends_on:
      - mongo
      - mcp-sse-proxy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - bricksllm-network
    command: ["npm", "run", "dev", "--", "--host", "0.0.0.0", "--port", "3000"]

  mcp-sse-proxy:
    build:
      context: ./mcp-sse-proxy
      dockerfile: Dockerfile
    container_name: mcp-sse-proxy
    restart: unless-stopped
    ports:
      - "3100:3100"
    volumes:
      # Configuration
      - ./mcp-sse-proxy/config:/app/config
      # Docker socket for mcp-server-docker
      - /var/run/docker.sock:/var/run/docker.sock
      # Persistent data for MCP servers
      - mcp_memory_data:/app/data/memory
      - mcp_uploads:/app/uploads
      - mcp_sandbox:/app/sandbox
    environment:
      - PORT=3100
      - CONFIG_PATH=/app/config/servers.json
      - NODE_ENV=production
      # API Keys for external MCP services
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY:-}
      - TAVILY_API_KEY=${TAVILIY_SEARCH_API_KEY:-}
      - SMITHERY_API_KEY=${SMITHERY_API_KEY:-}
    healthcheck:
      test: CMD curl -f http://localhost:3100/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - bricksllm-network

networks:
  bricksllm-network:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET:-172.30.0.0/16}

volumes:
  redis_data:
    driver: local
  postgresql_data:
    driver: local
  mongo_data:
    driver: local
  huggingface_cache:
    driver: local
  vllm_logs:
    driver: local
  bricksllm_logs:
    driver: local
  mcp_memory_data:
    driver: local
  mcp_uploads:
    driver: local
  mcp_sandbox:
    driver: local
