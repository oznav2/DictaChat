# Env Audit Report Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Generate a comprehensive `env_audit.md` that documents every environment variable used by the app (code + .env + Dockerfiles + docker-compose), including defaults, purpose, latency impact, and variants.

**Architecture:** Build the report from a structured inventory pipeline: extract env vars from `.env`, code (`$env/*`, `process.env`, `os.Getenv`, `os.getenv`), Dockerfiles (`ARG`/`ENV`), and `docker-compose.yml`, then merge into a single canonical list with per-service grouping. Add concise human descriptions and latency impact notes based on code usage and service role.

**Tech Stack:** SvelteKit (TypeScript), Go gateway, Python tools, Docker Compose, llama.cpp, MongoDB/Qdrant/Redis/PostgreSQL.

---

### Task 1: Inventory all env-variable sources (files + services)

**Files:**
- Read: `.env`
- Read: `docker-compose.yml`, `docker-compose-bak.yml` (if still referenced)
- Read: `Dockerfile`, `Dockerfile.*`, `frontend-huggingface/Dockerfile`
- Read: `start*.sh`, `*.sh` in root and `scripts/`
- Read: `frontend-huggingface/.env*` (if still present)

**Step 1: List candidate sources**
Run:
```bash
rg --files -g "*.env*" -g "Dockerfile*" -g "docker-compose*.yml" -g "*.sh" | rg -v "node_modules|dist|build"
```
Expected: list of env-containing files.

**Step 2: Capture docker-compose service list**
Run:
```bash
yq '.services | keys' docker-compose.yml
```
Expected: service names for grouping.

**Step 3: Create a scratch inventory directory**
Run:
```bash
mkdir -p /tmp/env-audit
```
Expected: empty dir for intermediate JSON/CSV.

---

### Task 2: Extract env vars from `.env` (values + defaults)

**Files:**
- Read: `.env`
- Create: `/tmp/env-audit/env.json`

**Step 1: Parse `.env` to structured JSON**
Run:
```bash
python - <<'PY'
import json, pathlib
env = {}
for line in pathlib.Path('.env').read_text().splitlines():
    line=line.strip()
    if not line or line.startswith('#') or '=' not in line:
        continue
    k,v=line.split('=',1)
    env[k.strip()] = v.strip()
pathlib.Path('/tmp/env-audit/env.json').write_text(json.dumps(env, indent=2))
print('vars', len(env))
PY
```
Expected: `/tmp/env-audit/env.json` with all `.env` keys.

---

### Task 3: Extract env vars from code (SvelteKit/Go/Python)

**Files:**
- Read: `frontend-huggingface/src/**`
- Read: `internal/**`, `cmd/**`, `datagov/**`, `scripts/**`
- Create: `/tmp/env-audit/code_env.json`

**Step 1: SvelteKit env usage**
Run:
```bash
rg -n "\$env/|process\.env" frontend-huggingface/src -g"*.ts" -g"*.svelte"
```
Expected: list of env usages.

**Step 2: Go env usage**
Run:
```bash
rg -n "os\.Getenv\(|os\.LookupEnv\(" internal cmd -g"*.go"
```
Expected: list of Go env usages.

**Step 3: Python env usage**
Run:
```bash
rg -n "os\.getenv\(|os\.environ\[" datagov scripts -g"*.py"
```
Expected: list of Python env usages.

**Step 4: Aggregate into JSON**
Run:
```bash
python - <<'PY'
import json, pathlib, re
root = pathlib.Path('.')
patterns = [
    (re.compile(r"process\.env\.([A-Z0-9_]+)"), ['frontend-huggingface/src']),
    (re.compile(r"env\.([A-Z0-9_]+)"), ['frontend-huggingface/src']),
    (re.compile(r"os\.Getenv\(\"([A-Z0-9_]+)\"\)"), ['internal','cmd']),
    (re.compile(r"os\.LookupEnv\(\"([A-Z0-9_]+)\"\)"), ['internal','cmd']),
    (re.compile(r"os\.getenv\(\"([A-Z0-9_]+)\"\)"), ['datagov','scripts']),
]
vars=set()
for regex, roots in patterns:
    for r in roots:
        for p in root.joinpath(r).rglob('*'):
            if p.suffix not in {'.ts','.svelte','.go','.py'}:
                continue
            try:
                text=p.read_text(encoding='utf-8')
            except Exception:
                continue
            for m in regex.finditer(text):
                vars.add(m.group(1))

pathlib.Path('/tmp/env-audit/code_env.json').write_text(json.dumps(sorted(vars), indent=2))
print('code vars', len(vars))
PY
```
Expected: `/tmp/env-audit/code_env.json` with unique env vars from code.

---

### Task 4: Extract env vars from Dockerfiles and docker-compose

**Files:**
- Read: `Dockerfile*`, `frontend-huggingface/Dockerfile`, `docker-compose.yml`
- Create: `/tmp/env-audit/docker_env.json`

**Step 1: Extract ARG/ENV from Dockerfiles**
Run:
```bash
rg -n "^(ARG|ENV) " -g"Dockerfile*" -g"frontend-huggingface/Dockerfile"
```
Expected: list of Docker build-time/runtime envs.

**Step 2: Extract docker-compose environment blocks**
Run:
```bash
yq '.services[].environment' docker-compose.yml
```
Expected: service-level env blocks.

**Step 3: Aggregate to JSON**
Run:
```bash
python - <<'PY'
import json, pathlib, re
root = pathlib.Path('.')
vars=set()
for p in root.rglob('Dockerfile*'):
    try:
        text=p.read_text()
    except Exception:
        continue
    for line in text.splitlines():
        line=line.strip()
        if line.startswith('ARG '):
            name=line.split()[1].split('=')[0]
            vars.add(name)
        if line.startswith('ENV '):
            parts=line.split()[1:]
            for part in parts:
                if '=' in part:
                    vars.add(part.split('=')[0])

compose = root/'docker-compose.yml'
if compose.exists():
    text = compose.read_text()
    # naive parse of KEY=VAL in environment lists
    for m in re.finditer(r"\n\s*-\s*([A-Z0-9_]+)=", text):
        vars.add(m.group(1))
    for m in re.finditer(r"\n\s*([A-Z0-9_]+):\s*\$\{", text):
        vars.add(m.group(1))

pathlib.Path('/tmp/env-audit/docker_env.json').write_text(json.dumps(sorted(vars), indent=2))
print('docker vars', len(vars))
PY
```
Expected: `/tmp/env-audit/docker_env.json` with unique docker env vars.

---

### Task 5: Merge inventories and compute defaults/variants

**Files:**
- Read: `/tmp/env-audit/env.json`, `/tmp/env-audit/code_env.json`, `/tmp/env-audit/docker_env.json`
- Create: `/tmp/env-audit/merged.json`

**Step 1: Merge to canonical list**
Run:
```bash
python - <<'PY'
import json, pathlib
env = json.loads(pathlib.Path('/tmp/env-audit/env.json').read_text())
code = set(json.loads(pathlib.Path('/tmp/env-audit/code_env.json').read_text()))
docker = set(json.loads(pathlib.Path('/tmp/env-audit/docker_env.json').read_text()))
all_vars = sorted(set(env.keys()) | code | docker)
merged = {k:{
    'in_env': k in env,
    'value': env.get(k),
    'in_code': k in code,
    'in_docker': k in docker,
} for k in all_vars}
pathlib.Path('/tmp/env-audit/merged.json').write_text(json.dumps(merged, indent=2))
print('total vars', len(all_vars))
PY
```
Expected: `/tmp/env-audit/merged.json` with all variables and presence flags.

---

### Task 6: Author `env_audit.md`

**Files:**
- Create: `env_audit.md`

**Step 1: Create report skeleton**
Include sections:
- Overview (scope + generation date)
- Global defaults + safety notes
- Per-service env tables (Gateway, Frontend, Llama, Qdrant, Redis, Mongo, PostgreSQL, MCP proxy, Docling, Dicta retrieval, DataGov, NER)
- Missing-but-used envs
- Build-time ARG/ENV summary

**Step 2: Populate each variable**
For every variable in `merged.json`:
- Default (from code or Dockerfile; if missing, mark “not defined”)
- Purpose (short, 1–2 sentences)
- Latency impact (none/low/med/high, explain)
- Variants (allowed values or ranges, or “boolean: true|false”)

**Step 3: Note source & service**
Annotate where it is used (code path, compose, Dockerfile), and which service it applies to.

---

### Task 7: Completeness check + doc polish

**Files:**
- Modify: `env_audit.md`

**Step 1: Validate missing vars**
Compare merged list vs `.env` and ensure “missing” section includes any var used in code/docker but absent in `.env`.

**Step 2: Sanity pass**
Check formatting consistency, ensure no sensitive values are exposed (mask API keys where appropriate).

**Step 3: Optional commit**
```bash
git add env_audit.md
# optional
# git commit -m "docs: add env audit report"
```

---

## Notes
- Use `rg` for fast searches (per repo guidance).
- Avoid leaking secrets in `env_audit.md` — show presence, not full values, for API keys.
- If any env var is used only by Docker build and not runtime, mark as **build-time**.

---

Plan complete and saved to `docs/plans/2026-01-29-env-audit.md`. Two execution options:

1. Subagent-Driven (this session) – I dispatch a fresh subagent per task, review between tasks, fast iteration
2. Parallel Session (separate) – Open new session with executing-plans, batch execution with checkpoints

Which approach?
