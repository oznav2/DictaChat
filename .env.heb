# ============================================================================
# DICTALM Chat Docker Compose - הגדרת סביבה (גרסה עברית)
# ============================================================================
# העתק קובץ זה ל-.env והתאם את הערכים לפריסה שלך
# cp .env.template .env
# ============================================================================

# ============================================================================
# PostgreSQL - הגדרות
# ============================================================================
POSTGRESQL_USERNAME=postgres
POSTGRESQL_PASSWORD=your_secure_postgres_password_here
POSTGRESQL_DB=bricksllm
POSTGRESQL_HOST_PORT=5433
POSTGRESQL_READ_TIME_OUT=2s
POSTGRESQL_WRITE_TIME_OUT=1s

# ============================================================================
# Redis - הגדרות
# ============================================================================
REDIS_PASSWORD=your_secure_redis_password_here
REDIS_HOST_PORT=6380
REDIS_READ_TIME_OUT=1s
REDIS_WRITE_TIME_OUT=1s

# ============================================================================
# Llama.cpp - הגדרות
# ============================================================================
LLAMA_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda
LLAMA_HOST_PORT=5002
HF_REPO=VRDate/DictaLM-3.0-24B-Thinking-FP8-Q4_K_M-GGUF
HF_FILE=dictalm-3.0-24b-thinking-fp8-q4_k_m.gguf
LOCAL_MODEL_PATH=/path/to/your/local/model/directory
CONTEXT_SIZE=32768
N_GPU_LAYERS=99
SYSTEM_PROMPT="You are DictaLM, a helpful AI assistant developed by Dicta. When answering, you MUST first provide your internal reasoning wrapped in <think></think> tags, followed by your final response outside the tags. Example: <think>Internal reasoning here...</think> Final response here."
TEMPERATURE=0.4
TOP_P=0.95
REPEAT_PENALTY=1.1
NUM_PREDICT=2048
GPU_DEVICE_IDS=0
GPU_COUNT=1
NODE_ENV=production

# ============================================================================
# BricksLLM Gateway - הגדרות
# ============================================================================
BRICKSLLM_MODE=production
BRICKSLLM_ADMIN_PORT=8001
BRICKSLLM_PROXY_PORT=8002
BRICKSLLM_API_KEY=your_bricksllm_api_key_here

IN_MEMORY_DB_UPDATE_INTERVAL=5s

PROXY_TIMEOUT=600s

NUMBER_OF_EVENT_MESSAGE_CONSUMERS=3

STATS_PROVIDER=

AMAZON_SECRET_ARN=
AMAZON_REGION=

# ============================================================================
# Network - הגדרות רשת
# ============================================================================
NETWORK_SUBNET=172.28.0.0/16

# ============================================================================
# MCP-SSE-PROXY הגדרות
# ============================================================================

TAVILIY_SEARCH_API_KEY='tvly-YOUR_API_KEY'
PERPLEXITY_API_KEY='pplx-YOUR_API_KEY'
SMITHERY_API_KEY='YOUR_API_KEY'

# ============================================================================
# MCP הגדרות נוספות 
# ============================================================================

# להוספה או שינוי של שרתי MCP, מוסיף אותם ל-servers.json בתיקיית mcpo-sse-proxy
# דוגמה:
# "mcp_server_name": {
#       "type": "sse",
#       "url": "http://mcp-server:8008/sse",
#       "headers": {
#         "Authorization": "Bearer ${MCP_SERVER_TOKEN}",
#         "X-Client-ID": "mcpo-sse-proxy"
#       }
#     }


# להוספה או שינוי של שרתי MCP נוספים, מוסיף אותם ל-servers.json בתיקיית mcpo-sse-proxy
# דוגמה:
# "external_sse": {
#       "type": "sse",
#       "url": "http://external-mcp-service:8008/sse",
#       "headers": {
#         "Authorization": "Bearer ${EXTERNAL_SERVICE_TOKEN}",
#         "X-Client-ID": "mcpo-sse-proxy"
#       }
#     },
#     "external_http": {
#       "type": "streamable_http", 
#       "url": "http://another-service:8009/mcp",
#       "headers": {
#         "X-API-Key": "${HTTP_SERVICE_KEY}"
#       }
#     }

# ============================================================================
# הערות והסברים להגדרות (גרסה עברית)
# ============================================================================
#
# PostgreSQL:
#   - POSTGRESQL_PASSWORD: צור סיסמה מאובטחת לפריסת production
#     דוגמה: openssl rand -base64 32
#     ברירת מחדל: postgres (לפיתוח בלבד)
#   - POSTGRESQL_HOST_PORT: פורט חיצוני הממופה למארח (5433 כדי למנוע התנגשויות)
#   - פורט קונטיינר פנימי הוא תמיד 5432
#
# Redis:
#   - REDIS_PASSWORD: צור מחרוזת אקראית מאובטחת לפריסת production
#     דוגמה: openssl rand -base64 32
#     ברירת מחדל: eYVX7EwVmmxKPCDmwMtyKVge8oLd2t81 (לפיתוח בלבד)
#   - REDIS_HOST_PORT: פורט חיצוני הממופה למארח (6380 כדי למנוע התנגשויות)
#   - פורט קונטיינר פנימי הוא תמיד 6379
#
# Llama.cpp:
#   - LLAMA_IMAGE: תמונת Docker לשרת Llama.cpp
#     ברירת מחדל: ghcr.io/ggml-org/llama.cpp:server-cuda
#     אלטרנטיבות: ghcr.io/ggml-org/llama.cpp:server-cpu (ל-CPU בלבד)
#
#   - HF_REPO: מאגר HuggingFace להורדת מודלים
#     דוגמה: VRDate/DictaLM-3.0-24B-Thinking-FP8-Q4_K_M-GGUF
#     אלטרנטיבות: TheBloke/Llama-2-7B-Chat-GGUF, TheBloke/Mistral-7B-Instruct-v0.2-GGUF
#
#   - HF_FILE: שם קובץ מודל ספציפי מהמאגר
#     דוגמה: dictalm-3.0-24b-thinking-fp8-q4_k_m.gguf
#     בדוק במאגר את שמות הקבצים הזמינים
#
#   - LOCAL_MODEL_PATH: נתיב מקומי לאחסון קבצי מודל
#     דוגמה: /home/user/.cache/models/VRDate/
#     המודל יורד למיקום זה אם אינו קיים
#
#   - CONTEXT_SIZE: אורך הקשר המרבי בטוקנים
#     ברירת מחדל: 8192 (8K context)
#     התאם לפי יכולות המודל: 4096, 8192, 16384, 32768
#
#   - N_GPU_LAYERS: מספר שכבות להעברה ל-GPU
#     ברירת מחדל: 99 (כל השכבות ב-GPU)
#     הגדר ל-0 להסקת מסקנות ב-CPU בלבד
#     ערכים נמוכים יותר מפחיתים שימוש בזיכרון GPU
#
#   - SYSTEM_PROMPT: הודעת מערכת למודל הבינה המלאכותית
#     התאם זאת כדי לשנות את התנהגות העוזר
#     שמור על פורמט ההיגיון אם משתמש במודלי DictaLM
#
#   - TEMPERATURE: טמפרטורת דגימה (0.0 עד 2.0)
#     ברירת מחדל: 0.7 (יצירתיות מאוזנת)
#     נמוך יותר = יותר דטרמיניסטי, גבוה יותר = יותר יצירתי
#
#   - TOP_P: פרמטר דגימת גרעין (0.0 עד 1.0)
#     ברירת מחדל: 0.9
#     נמוך יותר = יותר ממוקד, גבוה יותר = יותר מגוון
#
#   - REPEAT_PENALTY: קנס על טוקנים חוזרים (1.0 עד 2.0)
#     ברירת מחדל: 1.1
#     גבוה יותר = פחות חזרות
#
#   - NUM_PREDICT: מספר מרבי של טוקנים ליצירה
#     ברירת מחדל: 2048
#     הגדר ל-1- ליצירה ללא הגבלה (השתמש בזהירות)
#
#   - GPU_DEVICE_IDS: מזהה GPU מופרד בפסיקים (לדוגמה: "0,1")
#     ברירת מחדל: 0 (GPU יחיד)
#     השתמש ב-"0,1,2,3" להגדרות multi-GPU
#
#   - GPU_COUNT: מספר כרטיסי GPU לשימוש
#     חייב להתאים לספירה ב-GPU_DEVICE_IDS
#     ברירת מחדל: 1
#
# BricksLLM Gateway:
#   - BRICKSLLM_API_KEY: מפתח API לאימות BricksLLM
#     צור מפתח מאובטח לשימוש production
#     דוגמה: openssl rand -base64 32
#     מפתח זה צריך להישמר בסוד
#
#   - BRICKSLLM_MODE: מצב פריסה
#     אפשרויות: development, production
#     ברירת מחדל: production
#
#   - BRICKSLLM_ADMIN_PORT: פורט API מנהל (ניהול מפתחות, הגדרות ספק)
#     ברירת מחדל: 8001
#
#   - BRICKSLLM_PROXY_PORT: פורט API proxy (ניתוב בקשות LLM)
#     ברירת מחדל: 8002
#
#   - IN_MEMORY_DB_UPDATE_INTERVAL: מרווח זמן רענון מטמון
#     ברירת מחדל: 5s
#     הגדל לביצועים טובים יותר, הקטן לעדכונים מהירים יותר
#
# הערות אבטחה:
#   - לעולם אל תבצע commit לסיסמאות או מפתחות API אמיתיים לבקרת גרסאות
#   - צור סיסמאות ייחודיות ומאובטחות לכל פריסה
#   - השתמש בקבצי .env ספציפיים לסביבה (לדוגמה: .env.production, .env.development)
#   - שקול שימוש בשירותי ניהול סודות ל-production
#
# דוגמאות ליצירת סיסמאות מאובטחות:
#   PostgreSQL: openssl rand -base64 32
#   Redis: openssl rand -base64 32  
#   BricksLLM API Key: openssl rand -base64 32
#
# ============================================================================
# הוראות שימוש:
# 1. העתק קובץ זה: cp .env.heb .env
# 2. ערוך את הקובץ .env והחלף את כל הערכים עם "your_..._here" בערכים האמיתיים שלך
# 3. עבור סיסמאות ומפתחות, השתמש בפקודות openssl rand -base64 32 ליצירת ערכים מאובטחים
# 4. שמור את הקובץ .env - קובץ זה מכיל מידע רגיש ואל תבצע לו commit
# 5. הפעל את המערכת עם: ./start.sh
#
# ============================================================================