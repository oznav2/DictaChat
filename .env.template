# ============================================================================
# DICTALM Chat Docker Compose Environment Configuration
# ============================================================================
# Copy this file to .env and customize the values for your deployment
# cp .env.template .env
# ============================================================================

# ============================================================================
# PostgreSQL Configuration
# ============================================================================
POSTGRESQL_USERNAME=postgres
POSTGRESQL_PASSWORD=your_secure_postgres_password_here
POSTGRESQL_DB=bricksllm
POSTGRESQL_HOST_PORT=5433
POSTGRESQL_READ_TIME_OUT=2s
POSTGRESQL_WRITE_TIME_OUT=1s

# ============================================================================
# Redis Configuration
# ============================================================================
REDIS_PASSWORD=your_secure_redis_password_here
REDIS_HOST_PORT=6380
REDIS_READ_TIME_OUT=1s
REDIS_WRITE_TIME_OUT=1s

# ============================================================================
# Llama.cpp Configuration
# ============================================================================
LLAMA_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda
LLAMA_HOST_PORT=5002
HF_REPO=VRDate/DictaLM-3.0-24B-Thinking-FP8-Q4_K_M-GGUF
HF_FILE=dictalm-3.0-24b-thinking-fp8-q4_k_m.gguf
LOCAL_MODEL_PATH=/path/to/your/local/model/directory
CONTEXT_SIZE=8192
N_GPU_LAYERS=99
SYSTEM_PROMPT="You are DictaLM, a helpful AI assistant developed by Dicta. When answering, you MUST first provide your internal reasoning wrapped in <think></think> tags, followed by your final response outside the tags. Example: <think>Internal reasoning here...</think> Final response here."
TEMPERATURE=0.7
TOP_P=0.9
REPEAT_PENALTY=1.1
NUM_PREDICT=2048
GPU_DEVICE_IDS=0
GPU_COUNT=1
NODE_ENV=production

# ============================================================================
# BricksLLM Gateway Configuration
# ============================================================================
BRICKSLLM_MODE=production
BRICKSLLM_ADMIN_PORT=8001
BRICKSLLM_PROXY_PORT=8002
BRICKSLLM_API_KEY=your_bricksllm_api_key_here

# ============================================================================
# MCP Safety (Frontend)
# ============================================================================
MCP_ALLOW_LOCALHOST_URLS=false
MCP_ALLOW_PRIVATE_URLS=false
MCP_ALLOW_RESERVED_URLS=false
MCP_ALLOWED_HOSTS=
MCP_DEBUG_LOG_TOOL_OUTPUTS=false

# MCP API Keys
PERPLEXITY_API_KEY=
TAVILY_SEARCH_API_KEY=
SMITHERY_API_KEY=

# DataGov Proxy (Optional)
# Format: http://user:pass@host:port or http://host:port
DATAGOV_PROXY_URL=

IN_MEMORY_DB_UPDATE_INTERVAL=5s

PROXY_TIMEOUT=600s

NUMBER_OF_EVENT_MESSAGE_CONSUMERS=3

STATS_PROVIDER=

AMAZON_SECRET_ARN=
AMAZON_REGION=

# ============================================================================
# Network Configuration
# ============================================================================
NETWORK_SUBNET=172.28.0.0/16

# ============================================================================
# Retrieval (Embeddings & Reranking) Model Paths (inside container)
# ============================================================================
# Ensure models are mounted into the container:
#   volumes:
#     - ./.models:/app/models
# Example files under ./.models on host:
#   - /app/models/embeddings/bge-m3-f16.gguf
#   - /app/models/reranking/bge-reranker-v2-m3-q8_0.gguf
EMBEDDING_MODEL_NAME=/app/models/embeddings/bge-m3-f16.gguf
RERANKER_MODEL_NAME=/app/models/reranking/bge-reranker-v2-m3-q8_0.gguf

# ============================================================================
# Configuration Notes & Examples
# ============================================================================
#
# PostgreSQL:
#   - POSTGRESQL_PASSWORD: Generate a secure password for production
#     Example: openssl rand -base64 32
#     Default: postgres (development only)
#   - POSTGRESQL_HOST_PORT: External port mapped to host (5433 to avoid conflicts)
#   - Internal container port is always 5432
#
# Redis:
#   - REDIS_PASSWORD: Generate a secure random string for production
#     Example: openssl rand -base64 32
#     Default: eYVX7EwVmmxKPCDmwMtyKVge8oLd2t81 (development only)
#   - REDIS_HOST_PORT: External port mapped to host (6380 to avoid conflicts)
#   - Internal container port is always 6379
#
# Llama.cpp:
#   - LLAMA_IMAGE: Docker image for Llama.cpp server
#     Default: ghcr.io/ggml-org/llama.cpp:server-cuda
#     Alternatives: ghcr.io/ggml-org/llama.cpp:server-cpu (CPU-only)
#
#   - HF_REPO: HuggingFace repository for model download
#     Example: VRDate/DictaLM-3.0-24B-Thinking-FP8-Q4_K_M-GGUF
#     Alternatives: TheBloke/Llama-2-7B-Chat-GGUF, TheBloke/Mistral-7B-Instruct-v0.2-GGUF
#
#   - HF_FILE: Specific model file name from the repository
#     Example: dictalm-3.0-24b-thinking-fp8-q4_k_m.gguf
#     Check repository for available file names
#
#   - LOCAL_MODEL_PATH: Local path where model files are stored
#     Example: /home/user/.cache/models/VRDate/
#     Model will be downloaded to this location if not present
#
#   - CONTEXT_SIZE: Maximum context length in tokens
#     Default: 8192 (8K context)
#     Adjust based on model capabilities: 4096, 8192, 16384, 32768
#
#   - N_GPU_LAYERS: Number of layers to offload to GPU
#     Default: 99 (all layers on GPU)
#     Set to 0 for CPU-only inference
#     Lower values reduce GPU memory usage
#
#   - SYSTEM_PROMPT: System prompt for the AI model
#     Customize this to change the assistant's behavior
#     Keep the reasoning format if using DictaLM models
#
#   - TEMPERATURE: Sampling temperature (0.0 to 2.0)
#     Default: 0.7 (balanced creativity)
#     Lower = more deterministic, Higher = more creative
#
#   - TOP_P: Nucleus sampling parameter (0.0 to 1.0)
#     Default: 0.9
#     Lower = more focused, Higher = more diverse
#
#   - REPEAT_PENALTY: Penalty for repeated tokens (1.0 to 2.0)
#     Default: 1.1
#     Higher = less repetition
#
#   - NUM_PREDICT: Maximum number of tokens to generate
#     Default: 2048
#     Set to -1 for unlimited generation (use with caution)
#
#   - GPU_DEVICE_IDS: Comma-separated GPU IDs (e.g., "0,1")
#     Default: 0 (single GPU)
#     Use "0,1,2,3" for multi-GPU setups
#
#   - GPU_COUNT: Number of GPUs to use
#     Must match the count in GPU_DEVICE_IDS
#     Default: 1
#
# BricksLLM Gateway:
#   - BRICKSLLM_API_KEY: API key for BricksLLM authentication
#     Generate a secure key for production use
#     Example: openssl rand -base64 32
#     This key should be kept secret
#
#   - BRICKSLLM_MODE: Deployment mode
#     Options: development, production
#     Default: production
#
#   - BRICKSLLM_ADMIN_PORT: Admin API port (key management, provider settings)
#     Default: 8001
#
#   - BRICKSLLM_PROXY_PORT: Proxy API port (LLM request routing)
#     Default: 8002
#
#   - IN_MEMORY_DB_UPDATE_INTERVAL: Cache refresh interval
#     Default: 5s
#     Increase for better performance, decrease for faster updates
#
# Security Notes:
#   - NEVER commit actual passwords or API keys to version control
#   - Generate unique, secure passwords for each deployment
#   - Use environment-specific .env files (e.g., .env.production, .env.development)
#   - Consider using secret management services for production
#
# Example secure password generation:
#   PostgreSQL: openssl rand -base64 32
#   Redis: openssl rand -base64 32  
#   BricksLLM API Key: openssl rand -base64 32
#
# ============================================================================
